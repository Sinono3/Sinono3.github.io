<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns#">
<head>
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer-when-downgrade">

    <title>A Brief, Practical Introduction to Feature Visualization in PyTorch - Aldo Acevedo</title>
    <meta name="description" content="Guy who did software engineering, graphics programming, game development.
__Incursioning in machine learning and mechanistic interpretability.__
[Learn more about me.](&#x2F;about)

[**Esta página está disponible en Español.**](&#x2F;es)
">

    
    <link rel="stylesheet" href="https://sinono3.github.io/base.css">
    <link rel="stylesheet" href="https://sinono3.github.io/light.css">

    <link rel="preload" href=https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;font-awesome&#x2F;6.3.0&#x2F;css&#x2F;all.min.css as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href=https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;font-awesome&#x2F;6.3.0&#x2F;css&#x2F;all.min.css></noscript>

    

    
    
    <meta property="og:locale" content="en_US">
    
    <meta property="og:title" content="A Brief, Practical Introduction to Feature Visualization in PyTorch">
    
    <meta property="og:type" content="article">
    <meta property="article:author" content="https://sinono3.github.io/about/">
    <meta property="article:published_time" content="2024-09-27">
    
    
    <meta property="article:tag" content="interpretability">
    
    <meta property="article:tag" content="ml">
    
    
    
    
    <meta property="og:url" content="https://sinono3.github.io/blog/feature/">
    
    


    
    
</head>
<body>
<div class="container">
    
    
    <header>
        <h1 class="site-header">
            <a href="https:&#x2F;&#x2F;sinono3.github.io">Aldo Acevedo</a>
        </h1>
        <nav>
            <ul>
            
            
                
                <li><a class="active" href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;blog">Blog</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;projects">Projects</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;games">Games</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;music">Music</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;about">About</a></li>
            
            
            </ul>
        </nav>
    </header>
    
    

<article class="post">
    
    
    
        
        <header>
            <h1>A Brief, Practical Introduction to Feature Visualization in PyTorch</h1>
        </header>
        
    <div class="article-info">
        
        
        <div class="article-date">September 27, 2024 ◦ 16 min ◦ </div>
        
        <div class="article-taxonomies">
            
            
                <ul class="article-tags">
                    
                    <li><a href="https://sinono3.github.io/tags/interpretability/">#interpretability</a></li>
                    
                    <li><a href="https://sinono3.github.io/tags/ml/">#ml</a></li>
                    
                </ul>
            
        </div>
    </div>

    
    
    <div class="content">
        <blockquote>
<p><a href="/es/blog/feature"><strong>Este post está disponible en Español.</strong></a></p>
</blockquote>


<figure >
    <video  autoplay  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;castle_sped.mp4 type="video/mp4"></video>
    <figcaption><p>Visualization of castle (483)</p>
</figcaption>
</figure>

<p>Growing more capable by the second, AI is being adopted by both industries and governments as a mainstream technology.
AI allows automation of knowledge work. With enough data, compute, and algorithmic improvements, AI can replace engineering,
research, and administrative jobs.</p>
<p>This could be great or detrimental, depending on how effectively these technologies achieve our goals.
There's a bunch of reasons as to why this is, among them: </p>
<ol>
<li>We don't understand how deep learning models <em>actually</em> work.</li>
<li>We don't know how to make deep learning models do <em>precisely</em> what we want.</li>
<li>We don't even know what we want. (Or rather, how to explicitly state what we want)</li>
<li>We don't know which regulations reduce AI misuse most effectively.</li>
<li><em>...many more</em></li>
</ol>
<p>While all these problems are critical, this post focuses on problem 1: <strong>interpretability</strong>. I'm not an expert in this field, but I have <em>just</em> enough hands-on experience to lend a tiny hand to those looking to get started.</p>
<!-- TODO: You can check out these resources to answer the other questions... -->
<blockquote>
<p><strong>Heads-up</strong>: I assume some understanding of <a href="https://www.youtube.com/watch?v=aircAruvnKk">feed-forward neural networks (MLP)</a> and <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">gradient descent</a>.</p>
</blockquote>
<h2 id="what-does-interpretability-solve">What does interpretability solve?<a class="zola-anchor" href="#what-does-interpretability-solve" aria-label="Anchor link for: what-does-interpretability-solve">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Let's go back to the problem statement:</p>
<blockquote>
<p>We don't understand how deep learning models <em>actually</em> work.</p>
</blockquote>
<p>What do I mean by &quot;how they <em>actually</em> work&quot;?</p>
<p>For decades, we have built different deep learning architectures such as multi-layer perceptrons; convolutional, residual and recurrent neural networks; LSTMs; transformers; and many more. During training, these machines encode patterns and algorithms inside their parameters (weights, biases, etc). We understand how the process works, but not the specific patterns and algorithms that emerge.</p>
<p>Once trained, <strong>models are black boxes</strong>: while we provide inputs and get mostly correct outputs, we don't understand how the model did so at a neuron level. We know that neurons are connected, but <em>how</em> are they connected to achieve the goal?</p>
<p>I'll present you with an example of what I mean. Consider <a href="https://unsplash.com/photos/black-and-white-cat-lying-on-brown-bamboo-chair-inside-room-gKXKBY-C-Dk">this cat</a>.</p>









    



    
    
    

    
        
    

<figure >
    <img src="https:&#x2F;&#x2F;sinono3.github.io&#x2F;processed_images&#x2F;cat.65e72f53dcc5d2ac.jpg"  />
    <figcaption><p><a href="https://aisafety.dance/"><em>(Consider them.)</em></a></p>
</figcaption>
</figure>
 
<p>If fed to an image classification model such as ResNet18, it would be classified as &quot;Egyptian cat,&quot; which wouldn't be far from the truth. There's no single &quot;cat&quot; category in the model's output categories, so it would be impossible for the model to simply answer &quot;cat.&quot; So, practically, it's correct!</p>
<figure class="extended-figure">
    <img src="/blog/feature/ResNet18.excalidraw.svg" />
</figure>
<p>But how did the model come to that conclusion? Let's do a reverse analysis.</p>
<ul>
<li>Since the ImageNet class &quot;Egyptian cat&quot; has an index of 285, we know that in the fully connected last layer of the model (<code>fc</code>), neuron 285 has the greatest activation among the neurons in that layer (which are 1000 in total). This is because the last operation (<code>argmax</code>) returns the index of the previous layer's neuron with the greatest activation.</li>
<li>Neuron 285 in <code>fc</code> was activated because of some neuron activations in the previous layer (<code>avgpool</code>).</li>
<li>Neurons in <code>avgpool</code> that contributed to neuron 285 in <code>fc</code> come from the results of a convolutional layer. </li>
<li>This convolutional layer calculated its output using another convolutional layer's output.</li>
<li>And so on... until we get to the first layer, which is connected directly to the input image (cute cat pic).</li>
</ul>
<p>From this analysis, multiple questions pop up:</p>
<ol>
<li><strong>Circuit identification:</strong> Which neurons in <code>avgpool</code>, when activated, cause neuron 285 of layer <code>fc</code> to activate? And in the previous layer? What complex circuit of neurons has formed across the network's layers to conclude this image corresponds to an &quot;Egyptian cat&quot;?</li>
<li><strong>Visualization:</strong> What do these neurons firing represent? Do they correspond to concepts, shapes, forms, or objects? Can we see an image of what a single neuron represents? What about a set of neurons?</li>
<li><strong>Attribution</strong>: What parts of the input image contributed to the model outputting &quot;Egyptian cat&quot;? Which ones didn't? What parts of the input image contribute to a particular neuron firing? What images make a neuron fire?</li>
</ol>
<p>At the surface level, interpretability tries to answer these kinds of questions.</p>
<p>Today, we're going to have our try at <strong>visualization</strong>.</p>
<h2 id="defining-visualization">Defining visualization<a class="zola-anchor" href="#defining-visualization" aria-label="Anchor link for: defining-visualization">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>In the general case of any network, we can define <em>feature visualization</em> as generating an input that maximizes the activation of a part of the network: an output neuron, a hidden-layer neuron, a set of neurons, or an entire layer.</p>
<p>In the case of image classification models, feature <em>visualization</em> quite literally refers to generating an image. Let's say we do <em>class</em> visualization, where we optimize an image so the model <em>overwhelmingly</em> classifies it in a particular class (meaning the neuron corresponding to that class in the last layer will be significantly activated, more than all the other output neurons.) </p>
<p>In a perfect world, if we were to visualize class 285 on ResNet18, we would get an image of a cute kitten. In reality, though, feature visualizations can be confusing and unintelligible compared to a natural picture. We'll see this as we try to implement it ourselves.</p>
<h2 id="implementing-visualization">Implementing visualization<a class="zola-anchor" href="#implementing-visualization" aria-label="Anchor link for: implementing-visualization">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Using PyTorch, let's implement class visualization for a pre-trained image classification model. We're going to choose a specific ImageNet class and optimize an image so the model classifies it in the specified class. So, which class are we choosing?</p>









    



    
    
    

    
        
    

<figure >
    <img src="https:&#x2F;&#x2F;sinono3.github.io&#x2F;processed_images&#x2F;hen.7c53653065aed7b3.jpg"  />
    <figcaption><p>ImageNet class 8: <em>hen</em>. <a href="https://unsplash.com/photos/brown-and-red-he-n-G61iAuzI9NQ">Source.</a></p>
</figcaption>
</figure>
 
<p>Why chickens? Because <strong>all</strong> of them they easily recognizable red caruncles (look it up). Thus, it will be easier to see if our visualization works at all from the get-go.</p>
<blockquote>
<p><strong>In case you want to use another ImageNet class</strong>, <a href="https://github.com/pytorch/hub/blob/c7895df70c7767403e36f82786d6b611b7984557/imagenet_classes.txt">here's the list you can choose from</a>. Once you did, record the line number of the label and subtract 1 to get the output neuron or class index. (This is because line numbers start at 1, while PyTorch tensors indexes do at 0)</p>
</blockquote>
<p>We're going to visualize the ResNet18 model. I obtained &quot;good&quot; results with this model during my own experimentation. Better visualizations can be obtained with larger models such as VGG19, but at the cost of optimization speed. In this case, I'm trading off better quality for quicker feedback loops, allowing easier experimentation.</p>
<h3 id="base-case-optimize-the-input-like-we-re-optimizing-model-parameters">Base case: Optimize the input like we're optimizing model parameters<a class="zola-anchor" href="#base-case-optimize-the-input-like-we-re-optimizing-model-parameters" aria-label="Anchor link for: base-case-optimize-the-input-like-we-re-optimizing-model-parameters">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>We will begin writing code by importing matplotlib as our backend to display images conveniently. We'll also define a function <code>ptimg_to_mplimg</code> to convert the images from a PyTorch tensor to a numpy array, suitable for display in matplotlib. We define <code>show_img</code> to be able to display images concisely in a single function call.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>torch
</span><span style="color:#b48ead;">import </span><span>torchvision
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span>
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">ptimg_to_mplimg</span><span>(</span><span style="color:#bf616a;">input</span><span>: torch.Tensor):
</span><span>    </span><span style="color:#b48ead;">with </span><span>torch.</span><span style="color:#bf616a;">no_grad</span><span>():
</span><span>        </span><span style="color:#b48ead;">return </span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">detach</span><span>().</span><span style="color:#bf616a;">squeeze</span><span>().</span><span style="color:#bf616a;">permute</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">2</span><span>, </span><span style="color:#d08770;">0</span><span>).</span><span style="color:#bf616a;">clamp</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#d08770;">1</span><span>).</span><span style="color:#bf616a;">numpy</span><span>()
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">show_img</span><span>(</span><span style="color:#bf616a;">input</span><span>: torch.Tensor, </span><span style="color:#bf616a;">title</span><span>: str):
</span><span>    plt.</span><span style="color:#bf616a;">imshow</span><span>(</span><span style="color:#bf616a;">ptimg_to_mplimg</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    plt.</span><span style="color:#bf616a;">title</span><span>(title)
</span><span>    </span><span style="color:#65737e;"># Setting `block=False` and calling `plt.pause` allow us to display the progress in real time
</span><span>    plt.</span><span style="color:#bf616a;">show</span><span>(</span><span style="color:#bf616a;">block</span><span>=</span><span style="color:#d08770;">False</span><span>)
</span><span>    plt.</span><span style="color:#bf616a;">pause</span><span>(</span><span style="color:#d08770;">0.1</span><span>)
</span></code></pre>
<p>With the boilerplate out of the way, let's go ahead and implement the simplest, most obvious way to do class visualization. We'll refer to how we usually train neural networks: using a built-in PyTorch optimizer which adjusts parameters to minimize the loss function. Here, we will try to do the same, but instead of optimizing the model's parameters, we will optimize the input. &quot;What will be our loss function?&quot; you may ask. We'll answer that later.</p>
<p>Let's download the pre-trained model:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model = torch.hub.</span><span style="color:#bf616a;">load</span><span>(&quot;</span><span style="color:#a3be8c;">pytorch/vision:v0.10.0</span><span>&quot;, &quot;</span><span style="color:#a3be8c;">resnet18</span><span>&quot;, </span><span style="color:#bf616a;">weights</span><span>=&quot;</span><span style="color:#a3be8c;">ResNet18_Weights.IMAGENET1K_V1</span><span>&quot;)
</span><span style="color:#65737e;"># Set the model to evaluation mode: 
</span><span style="color:#65737e;"># Disables dropout and batch normalization layers, which we don&#39;t need right now
</span><span>model.</span><span style="color:#bf616a;">eval</span><span>()
</span></code></pre>
<p>We need to define our initial image, which will be the starting point for the optimization. We'll use uniformly random values ranging from 0 to 1. Do notice that we can use any image as the starting point.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">input </span><span>= torch.</span><span style="color:#bf616a;">rand</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">3</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>)
</span></code></pre>
<p>We declare our target class to be &quot;hen.&quot;</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">TARGET_CLASS </span><span>= </span><span style="color:#d08770;">8 </span><span style="color:#65737e;"># cluck cluck, buck buck bugawk
</span></code></pre>
<p>Hmm, what optimizer should we use? Why not plain ol' SGD? (stochastic gradient descent)</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">LR </span><span>= </span><span style="color:#d08770;">0.5
</span><span>optimizer = torch.optim.</span><span style="color:#bf616a;">SGD</span><span>([</span><span style="color:#96b5b4;">input</span><span>], </span><span style="color:#bf616a;">lr</span><span>=</span><span style="color:#bf616a;">LR</span><span>)
</span></code></pre>
<p>We'll create a function that performs a single optimization step to organize our code neatly.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#96b5b4;">input</span><span>)
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>
<p>Here, we have done something important: <strong>We defined our loss function as the negative of the activation of the output neuron corresponding to the target class</strong>. We want this neuron's activation to be as great as possible. Our optimizer tries to <em>minimize</em> the loss function, thus, if we set the loss function to be the negative of the target neuron's activation, the optimizer will try to maximize the target neuron's activation.</p>
<p>Now, we simply need to call the step function in a loop, displaying our image every few steps to see the progress.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">STEPS </span><span>= </span><span style="color:#d08770;">200
</span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#bf616a;">STEPS</span><span>):
</span><span>    </span><span style="color:#bf616a;">step</span><span>()
</span><span>
</span><span>    </span><span style="color:#65737e;"># Show image every 10 steps
</span><span>    </span><span style="color:#b48ead;">if </span><span>i % </span><span style="color:#d08770;">10 </span><span>== </span><span style="color:#d08770;">0</span><span>:
</span><span>        </span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Step </span><span>{i}</span><span style="color:#a3be8c;">/</span><span>{</span><span style="color:#bf616a;">STEPS</span><span>}&quot;)
</span><span>        </span><span style="color:#bf616a;">show_img</span><span>(</span><span style="color:#96b5b4;">input</span><span>, </span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Visualization of class </span><span>{</span><span style="color:#bf616a;">TARGET_CLASS</span><span>}&quot;)
</span></code></pre>
<p>Okay! We've completed our first version. Let's see how it does.</p>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app0.mp4 type="video/mp4"></video>
    
</figure>

<p>Hmm. That doesn't quite look like a fowl. What can we do to improve this?</p>
<h3 id="improvement-1-change-the-initial-image">Improvement 1: Change the initial image<a class="zola-anchor" href="#improvement-1-change-the-initial-image" aria-label="Anchor link for: improvement-1-change-the-initial-image">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Starting with random values from 0 to 1 may cause the optimizer to tend to extreme values (outside the 0-1 RGB range). Not only do these values generate high contrast in the image, but if we want to get rid of the noise, starting with an already noisy image may not be the best option. Let's try a more uniformly gray initial image. To be more precise, the same random noise, but with a mean of 0.5 and a range of 0.49-0.51.</p>
<p>Let's reflect these changes in our code:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">input </span><span>= (torch.</span><span style="color:#bf616a;">rand</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">3</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#d08770;">299</span><span>) - </span><span style="color:#d08770;">0.5</span><span>) * </span><span style="color:#d08770;">0.01 </span><span>+ </span><span style="color:#d08770;">0.5
</span><span style="color:#96b5b4;">input</span><span>.requires_grad = </span><span style="color:#d08770;">True
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app1.mp4 type="video/mp4"></video>
    
</figure>

<p>That's a lot better! If one squints, the red caruncles of the chickens pop out, while in the rest of the image, feather-like patterns start to emerge.</p>
<h3 id="improvement-2-enforcing-transformational-robustness">Improvement 2: Enforcing transformational robustness<a class="zola-anchor" href="#improvement-2-enforcing-transformational-robustness" aria-label="Anchor link for: improvement-2-enforcing-transformational-robustness">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>We've been calculating the gradients based on the same image at the same scale, rotation, and translation for every step.
This means our code optimizes for the image to be classified as &quot;hen&quot; only from one point of view.
If we rotate the image, it's not certain that it will still be classified as &quot;hen.&quot; 
Our image is not <em>transformationally robust</em>.</p>
<p>If we want the model to recognize the target class in our image after being scaled or rotated, we must optimize the image to do so.</p>
<blockquote>
<p><strong>Why would this help with the noise problem?</strong></p>
</blockquote>
<p>I don't have it quite clear, actually. 
Speaking <em>very vaguely</em>, introducing stochastic jittering, scale, and rotation seems to prevent the optimizer from sticking to a certain noisy pattern.
For a much better explanation, you can check <a href="https://distill.pub/2017/feature-visualization">the <em>Transformational robustness</em> section of the <em>Feature Visualization</em> paper on Distill.</a></p>
<p>Anyways, the implementation involves applying random transformations on the input before the optimization.</p>
<p>Before our step function, we must define what transforms we'll apply to our image before passing it to our model.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">from </span><span>torchvision </span><span style="color:#b48ead;">import </span><span>transforms </span><span style="color:#b48ead;">as </span><span>T
</span><span>transforms = T.</span><span style="color:#bf616a;">Compose</span><span>([
</span><span>    </span><span style="color:#65737e;"># Applies minor changes in brightness, contrast, saturation, and hue
</span><span>    T.</span><span style="color:#bf616a;">ColorJitter</span><span>(</span><span style="color:#bf616a;">brightness</span><span>=(</span><span style="color:#d08770;">0.98</span><span>, </span><span style="color:#d08770;">1.02</span><span>), </span><span style="color:#bf616a;">contrast</span><span>=(</span><span style="color:#d08770;">0.99</span><span>, </span><span style="color:#d08770;">1.01</span><span>), </span><span style="color:#bf616a;">saturation</span><span>=(</span><span style="color:#d08770;">0.98</span><span>, </span><span style="color:#d08770;">1.02</span><span>), </span><span style="color:#bf616a;">hue</span><span>=(-</span><span style="color:#d08770;">0.01</span><span>, </span><span style="color:#d08770;">0.01</span><span>)),
</span><span>    </span><span style="color:#65737e;"># Rotates the image by 15deg clockwise or counter-clockwise. We also apply a bit of random zoom in and out.
</span><span>    T.</span><span style="color:#bf616a;">RandomAffine</span><span>(</span><span style="color:#bf616a;">degrees</span><span>=(-</span><span style="color:#d08770;">15.0</span><span>, </span><span style="color:#d08770;">15.0</span><span>), </span><span style="color:#bf616a;">scale</span><span>=(</span><span style="color:#d08770;">0.96</span><span>,</span><span style="color:#d08770;">1.04</span><span>), </span><span style="color:#bf616a;">fill</span><span>=</span><span style="color:#d08770;">0.5</span><span>),
</span><span>])
</span></code></pre>
<p>On our step function, we must modify the forward-pass call.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))	</span><span style="color:#65737e;"># &lt;-- this line right here
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app2.mp4 type="video/mp4"></video>
    <figcaption><p>Result after optimizing for transformational robustness.</p>
</figcaption>
</figure>

<blockquote>
<p><strong>Note on gradient propagation:</strong> While I was originally implementing transformational robustness, I misunderstood its concept and <em>actually transformed the visualization</em>, instead of <em>just doing gradient propagation on</em> the transformed image. The difference is in the step function:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Case 1: ACTUALLY TRANSFORMING THE VISUALIZATION (don&#39;t try at home)
</span><span style="color:#96b5b4;">input </span><span>= </span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">detach</span><span>())
</span><span>output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#96b5b4;">input</span><span>)
</span><span style="color:#65737e;"># Case 2: JUST DOING GRADIENT PROPAGATION ON THE TRANSFORMED IMAGE
</span><span>output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span></code></pre>
</blockquote>
<h3 id="improvement-3-implement-l2-regularization">Improvement 3: Implement L2 regularization<a class="zola-anchor" href="#improvement-3-implement-l2-regularization" aria-label="Anchor link for: improvement-3-implement-l2-regularization">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>L2 regularization. Hmm, what? It's simply adding the square of the parameters we're optimizing to our loss function. Actually, we add the sum of the squares of our parameters multiplied by a coefficient, usually called λ (lambda). In this case, the parameters are the color values for each pixel.</p>
<p>This basically penalizes color values that deviate significantly from 0. In our case, we don't want values that stray too far from 0.5, the &quot;middle&quot; point between 0.0 and 1.0, the range of color values. This allows our optimization to have a balance between maximizing our target activation (be it class, neuron, layer, whatever) and having our image be in a valid color range. This will get rid of values that are too extreme on the red, green, or blue channels.</p>
<p>Implementing it is quite easy. We only need to define λ and change a line in the loss function definition.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">LR </span><span>= </span><span style="color:#d08770;">1.5 </span><span style="color:#65737e;"># We&#39;ll also increase the learning rate.
</span><span>L2_lambda = </span><span style="color:#d08770;">0.05
</span></code></pre>
<p>In our step function:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>() + L2_lambda * ((</span><span style="color:#96b5b4;">input </span><span>- </span><span style="color:#d08770;">0.5</span><span>) ** </span><span style="color:#d08770;">2</span><span>).</span><span style="color:#bf616a;">sum</span><span>()     </span><span style="color:#65737e;"># &lt;-- this line
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app3.mp4 type="video/mp4"></video>
    
</figure>

<h3 id="improvement-4-blur-the-image-every-few-steps">Improvement 4: Blur the image every few steps<a class="zola-anchor" href="#improvement-4-blur-the-image-every-few-steps" aria-label="Anchor link for: improvement-4-blur-the-image-every-few-steps">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Now, for a final technique, I'll introduce a somewhat obvious technique to get rid of noise, which can work surprisingly well: simply applying Gaussian blur to the image. Of course, if we do this repeatedly we will only get a blurry image. If done occasionally, though, we can obtain good results.</p>
<p>We will add a parameter to our step function so we can know our step index.
Arbitrarily, I've set the step function to blur the image every 10 steps.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>(</span><span style="color:#bf616a;">i</span><span>: int):
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>() + L2_lambda * ((</span><span style="color:#96b5b4;">input </span><span>- </span><span style="color:#d08770;">0.5</span><span>) ** </span><span style="color:#d08770;">2</span><span>).</span><span style="color:#bf616a;">sum</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span><span>
</span><span>    </span><span style="color:#b48ead;">if </span><span>i % </span><span style="color:#d08770;">10 </span><span>== </span><span style="color:#d08770;">0</span><span>:
</span><span>        </span><span style="color:#b48ead;">with </span><span>torch.</span><span style="color:#bf616a;">no_grad</span><span>():
</span><span>            </span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">copy_</span><span>((T.</span><span style="color:#bf616a;">GaussianBlur</span><span>(</span><span style="color:#d08770;">5</span><span>, </span><span style="color:#d08770;">1.5</span><span>))(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>
</span><span>
</span><span style="color:#bf616a;">STEPS </span><span>= </span><span style="color:#d08770;">200
</span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#bf616a;">STEPS</span><span>):
</span><span>    </span><span style="color:#bf616a;">step</span><span>(i)
</span><span>    </span><span style="color:#65737e;"># ... the rest of the code
</span></code></pre>
<p>This will coincide exactly with our image display, so you can see the blur effect.
To prevent this coincidence you can change the blurring condition to <code>i % 10 == 1</code>.
This will make the blurring occur exactly after displaying the image, instead of before.</p>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app4.mp4 type="video/mp4"></video>
    
</figure>

<h2 id="testing-our-visualization-on-various-classes">Testing our visualization on various classes<a class="zola-anchor" href="#testing-our-visualization-on-various-classes" aria-label="Anchor link for: testing-our-visualization-on-various-classes">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Now that we've got something working, let's try our model with lots of different classes:</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;conclusion.mp4 type="video/mp4"></video>
    <figcaption><p>From top-left to bottom-right: hen (8), pelican (144), leopard (288), hammer (587), iPod (605), slot (800), potpie (964), scuba diver (983).</p>
</figcaption>
</figure>

<h3 id="weird-experimentation-eternal-zoom-in">Weird experimentation (eternal zoom-in)<a class="zola-anchor" href="#weird-experimentation-eternal-zoom-in" aria-label="Anchor link for: weird-experimentation-eternal-zoom-in">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>While initially implementing transformational robustness, I misunderstood the concept and <em>actually transformed the visualization</em>, instead of <em>just doing gradient propagation on</em> the transformed image. During these trying times, I experimented with biasing the transformations to continually increase the scale of the image. The result is an eternal zoom-in effect, but the atmosphere is that of endlessly submerging yourself in alien worlds.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;alien3.mp4 type="video/mp4"></video>
    <figcaption><p>Left: 'hen' (8). Right: 'robin' (15)</p>
</figcaption>
</figure>

<p>I also implemented a sort-of L1 regularization where simply the color values multiplied by a lambda coefficient are added to the loss function. This gives some very trippy results.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;alien.mp4 type="video/mp4"></video>
    <figcaption><p>Left: 'hen' (8). Right: 'robin' (15)</p>
</figcaption>
</figure>

<p>I'd dare say that these moving visualizations give us a different perspective to understand what a feature represents.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;barbellwine.mp4 type="video/mp4"></video>
    <figcaption><p>Left: Barbell World. Right: Wine World.</p>
</figcaption>
</figure>

<h2 id="limitations">Limitations<a class="zola-anchor" href="#limitations" aria-label="Anchor link for: limitations">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>There are definitely some limitations to our current approach:</p>
<ul>
<li>L2 regularization inherently reduces color contrast since we unfavor color values that stray far from [0.5, 0.5, 0.5].</li>
<li>ResNet18 is a relatively small model, so its visualizations may not be as high quality as those from larger models.</li>
<li>ResNet18 seems to have a bias towards the color green. This is a common feature among many of the models I've tested. <em>My theory</em>: since many of the classes and thus training data are animal-related, grass is a common denominator in the background of these pictures. Therefore, the model sees a lot of green during its training, which generates a bias.</li>
<li><a href="https://en.wikipedia.org/wiki/Bilateral_filter">There are better alternatives to Gaussian blur for our purpose.</a></li>
</ul>
<p>Reading <a href="https://distill.pub/2017/feature-visualization/">this article will give you an idea of how good feature visualization can look when done right</a>.</p>
<h2 id="conclusion">Conclusion<a class="zola-anchor" href="#conclusion" aria-label="Anchor link for: conclusion">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>That's good enough for the scope of this post. I hope you found interpretability to be fun and interesting. </p>
<p>And hey, if you did, don't stop here! I barely scratched the surface of what the field actually revolves around. There are a bunch of resources for you to continue researching:</p>
<ul>
<li><a href="https://distill.pub/2020/circuits/">Distill's Circuits Thread</a>: Great and accessible interpretability papers. Contains <a href="https://distill.pub/2017/feature-visualization">a paper specifically related to feature visualizations</a>.</li>
<li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a>: State-of-the-art interpretability papers about transformers and language models.</li>
<li><a href="https://www.arena.education/">ARENA Course</a>: Hands-on practice for AI alignment/interpretability skills.</li>
</ul>
<!--
I apologize for a lack of formalities. At no point I provided any reference, proof or evidence of what I suggested throughout. However, I believe skipping a couple steps can sometimes be more effective to explain something to someone new to field. (I can confirm that, still being new to the field myself!)

## Prithvi's Notes

- Explaining the code more explicitly. That may help the more general audience understand exactly what they're doing.
- Maybe add "nutshells", where if people wanted to learn more about a topic, they could open a little box explainer. This is to add information where there could be, but also make it optional.
 -->

    </div>
    

    
</article>


    <footer>
        <p>
            
<span class="icon-text">
  

  

  
  <a href="https://github.com/Sinono3" aria-label="Open Sinono3 GitHub" target="_blank">
    <span class="icon is-large">
      <i class="fab fa-github fa-lg" aria-hidden="true" title="GitHub"></i>
    </span>
  </a>
  

  
  <a href="https://www.linkedin.com/in/aldo-acevedo-9a38a9289" aria-label="Open aldo-acevedo-9a38a9289 LinkedIn" target="_blank">
    <span class="icon is-large">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true" title="LinkedIn"></i>
    </span>
  </a>
  

  

  

  

  
  <a href="mailto:aldoacevedo@protonmail.com" aria-label="Email aldoacevedo@protonmail.com" target="_blank">
    <span class="icon is-large">
      <i class="far fa-envelope fa-lg" aria-hidden="true" title="email"></i>
    </span>
  </a>
  

  

  
</span>

        </p>
        <p>
            
            
        </p>
    </footer>
</div>
</body>
</html>
