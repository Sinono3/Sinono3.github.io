<!DOCTYPE html>
<html lang="es" prefix="og: http://ogp.me/ns#">
<head>
    <meta charset="utf-8">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer-when-downgrade">

    <title>Una breve y práctica introducción a la visualización de features en PyTorch - Aldo Acevedo</title>
    <meta name="description" content="Personaje que desarrolló software, gráficos y videojuegos.
__Ahora estoy incursionando en machine learning e interpretabilidad mecanística.__
[Conoce más acerca de mí.](&#x2F;es&#x2F;about)

[**This page is available in English.**](&#x2F;)
">

    
    <link rel="stylesheet" href="https://sinono3.github.io/base.css">
    <link rel="stylesheet" href="https://sinono3.github.io/light.css">

    <link rel="preload" href=https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;font-awesome&#x2F;6.3.0&#x2F;css&#x2F;all.min.css as="style" onload="this.onload=null;this.rel='stylesheet'">
    <noscript><link rel="stylesheet" href=https:&#x2F;&#x2F;cdnjs.cloudflare.com&#x2F;ajax&#x2F;libs&#x2F;font-awesome&#x2F;6.3.0&#x2F;css&#x2F;all.min.css></noscript>

    

    
    
    <meta property="og:locale" content="en_US">
    
    <meta property="og:title" content="Una breve y práctica introducción a la visualización de features en PyTorch">
    
    <meta property="og:type" content="article">
    <meta property="article:author" content="https://sinono3.github.io/about/">
    <meta property="article:published_time" content="2024-09-27">
    
    
    <meta property="article:tag" content="interpretability">
    
    <meta property="article:tag" content="ml">
    
    
    
    
    <meta property="og:url" content="https://sinono3.github.io/es/blog/feature/">
    
    


    
    
</head>
<body>
<div class="container">
    
    
    <header>
        <h1 class="site-header">
            <a href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es">Aldo Acevedo</a>
        </h1>
        <nav>
            <ul>
            
            
                
                <li><a class="active" href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es&#x2F;blog">Blog</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es&#x2F;projects">Proyectos</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es&#x2F;games">Jueguitos</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es&#x2F;music">Música</a></li>
            
                
                <li><a  href="https:&#x2F;&#x2F;sinono3.github.io&#x2F;es&#x2F;about">Acerca de</a></li>
            
            
            </ul>
        </nav>
    </header>
    
    

<article class="post">
    
    
    
        
        <header>
            <h1>Una breve y práctica introducción a la visualización de features en PyTorch</h1>
        </header>
        
    <div class="article-info">
        
        
        <div class="article-date">September 27, 2024 ◦ 16 min ◦ </div>
        
        <div class="article-taxonomies">
            
            
                <ul class="article-tags">
                    
                    <li><a href="https://sinono3.github.io/es/tags/interpretability/">#interpretability</a></li>
                    
                    <li><a href="https://sinono3.github.io/es/tags/ml/">#ml</a></li>
                    
                </ul>
            
        </div>
    </div>

    
    
    <div class="content">
        <blockquote>
<p><a href="/blog/feature"><strong>This post is available in English.</strong></a></p>
</blockquote>


<figure >
    <video  autoplay  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;castle_sped.mp4 type="video/mp4"></video>
    <figcaption><p>Visualización de 'castillo' (483)</p>
</figcaption>
</figure>

<p>A medida que la inteligencia artifical adquiere mayor capacidad, es adoptada por ambos la industria y el gobierno como una tecnología estándar.
La IA permite la automatización del trabajo del conocimiento. Con suficientes datos, poder computacional y mejoras algorítmicas, una IA podrá ocupar
puestos investigativos, administrativos o de ingeniería.</p>
<p>Dependiendo de cuánto estos nos ayuden a alcanzar nuestros objetivos, puede ser a beneficio o detrimento de la humanidad.
Actualmente hay muchos problemas obstruyendo el avance, entre ellos:</p>
<ol>
<li>No entendemos cómo <em>realmente</em> funcionan los modelos de deep learning.</li>
<li>No sabemos cómo hacer que los modelos de deep learning hagan <em>precisamente</em> lo que queremos.</li>
<li>No sabemos <em>qué</em> queremos. (O mejor dicho, definir precisa y explícitamente qué queremos)</li>
<li>No sabemos qué regulaciones son efectivas para reducir el mal uso de la IA.</li>
<li><em>Muchos más...</em></li>
</ol>
<p>Mientras que estos problemas son clave, este artículo se enfoca en el problema 1: <strong>interpretabilidad</strong>. No soy un experto en el campo, pero entiendo <em>justo lo suficiente</em> para dar una diminuta mano a aquellos que deseen aprender más del tema.</p>
<!-- TODO: You can check out these resources to answer the other questions... -->
<blockquote>
<p><strong>Aviso</strong>: Asumo un poco de entendimiento de <a href="https://www.youtube.com/watch?v=aircAruvnKk">redes neuronales feed-forward (MLP)</a> y <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">gradient descent</a>.</p>
</blockquote>
<h2 id="que-resuelve-la-interpretabilidad">¿Qué resuelve la interpretabilidad?<a class="zola-anchor" href="#que-resuelve-la-interpretabilidad" aria-label="Anchor link for: que-resuelve-la-interpretabilidad">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Volvamos al enunciado del problema.</p>
<blockquote>
<p>No entendemos cómo <em>realmente</em> funcionan los modelos de deep learning.</p>
</blockquote>
<p>A qué me refiero por &quot;¿cómo realmente funcionan?&quot;</p>
<p>Por decadas, hemos construído diferentes arquitecturas de deep learning, como multi-layer perceptrons; redes neuronales convolucionales, residuales y recurrentes; LSTMs; transformers; y muchos más. Durante el entrenamiento, estas máquinas codifican patrones y algoritmos dentro de sus parámetros (weights, biases, etc). Entendemos cómo funciona el proceso, pero no específicamente los patrones y algorítmos que emergen.</p>
<p>Una vez entrenado, <strong>los modelos son cajas negras</strong>: mientras les alimentemos inputs y recibamos outputs correctos, no entendemos como el modelo logró eso a un nivel neuronal. Sabemos que las neuronas están conectadas para eso, pero ¿<em>cómo</em>?</p>
<p>Presentaré un ejemplo para mostrar a qué me refiero. Considerá a <a href="https://unsplash.com/photos/black-and-white-cat-lying-on-brown-bamboo-chair-inside-room-gKXKBY-C-Dk">este gato</a>.</p>









    



    
    
    

    
        
    

<figure >
    <img src="https:&#x2F;&#x2F;sinono3.github.io&#x2F;processed_images&#x2F;cat.65e72f53dcc5d2ac.jpg"  />
    <figcaption><p><a href="https://aisafety.dance/"><em>(Dale. Considerale.)</em></a></p>
</figcaption>
</figure>
 
<p>Si alimentamos esta imagen a un modelo de clasificación como ResNet18, sería clasificado como &quot;Gato egipcio&quot;, lo cual no estaría lejos de la verdad.
No existe una categoría &quot;gato&quot; entre los posibles outputs del modelo, así que sería imposible que el modelo sencillamente responda &quot;gato.&quot; Practicamente, está correcto!</p>
<figure class="extended-figure">
    <img src="/blog/feature/ResNet18.excalidraw.svg" />
</figure>
<p>¿Pero cómo llegó el modelo a esa conclusión? Hagamos un análisis en reversa.</p>
<ul>
<li>Ya que la clase de ImageNet &quot;Gato egipcio&quot; tiene un índice 285, sabemos que en la última capa fully-connected del modelo (<code>fc</code>), la neurona 285 tiene la mayor activations de las neuronas de aquella capa (que son 1000 en total). Esto es por la última operacion (<code>argmax</code>) devuelve el índice de la neurona de la capa anterior con la mayor activación.</li>
<li>La neurona 285 de <code>fc</code> fue activada por algunas activaciones neuronales de la capa anterior (<code>avgpool</code>).</li>
<li>Neuronas de <code>avgpool</code> que contribuyeron a la neurona 285 de <code>fc</code> vienen del output de una capa convolucional.</li>
<li>Esta capa convolucional fue calculada con los resultados de otra capa convolucional.</li>
<li>Y así sucesívamente hasta que lleguemos a la primera capa convolucional, que está directamente conectada a nuestra imagen (gatito.)</li>
</ul>
<p>De este análisis surgen muchas preguntas:</p>
<ol>
<li><strong>Identificación de circuitos:</strong> ¿qué neuronas en <code>avgpool</code>, una vez activadas, activan por consecuente a la neurona 285 de la capa <code>fc</code>? ¿Y en la capa anterior? ¿Qué circuito complejo de neuronas se ha formado a través de las capas de la red para que el modelo llegue a la conclusión de que esta imagen corresponde a un &quot;Gato egipcio?&quot;</li>
<li><strong>Visualización:</strong> ¿qué representan los disparos de estas neuronas? ¿Corresponden a conceptos, formas u objetos? ¿Podemos ver una imagen de qué representa una sola neurona? ¿Qué hay acerca de un conjunto de neuronas?</li>
<li><strong>Atribución:</strong> ¿qué partes de la imagen original contribuyeron a que el modelo concluya &quot;Gato egipcio?&quot; ¿Qué partes no? ¿Qué partes de la imágen contribuyeron a una neurona particular a disparar? ¿Qué imágenes hacen que una neurona dispare?</li>
</ol>
<p>Hablando superficialmente, interpretabilidad intenta responder estas preguntas.</p>
<p>Hoy, vamos a intentar responder algunas preguntas sobre <em>visualización</em>.</p>
<h2 id="definiendo-visualizacion">Definiendo visualización<a class="zola-anchor" href="#definiendo-visualizacion" aria-label="Anchor link for: definiendo-visualizacion">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>En el caso general de una red neuronal, podemos definir <em>visualización de features</em> como generar un input que maximice la activación de una parte de la red: una neurona de output, una neurona de un hidden-layer, un conjunto de neuronas o una capa entera.</p>
<p>En el caso de un modelo de clasificación de imágenes, con <em>visualización</em> de features literalmente nos referimos a generar una imagen. Digamos que hacemos visualización de <em>clases</em>, donde optimizamos una imagen como para que el modelo lo clasifique dentro de una clase particular (significando que la neurona correspondiente a esa clase en la última capa será significativamente activada, más que todas las otras neuronas de output.)</p>
<p>In a perfect world, if we were to visualize class 285 on ResNet18, we would get an image of a cute kitten. In reality, though, feature visualizations can be confusing and unintelligible compared to a natural picture. We'll see this as we try to implement it ourselves.</p>
<p>En un mundo perfecto, si fueramos a visualizar la clase 285 en ResNet18 obtendríamos una imagen de un gatito. Sin embargo, las visualizaciones de features en realidad tienden a ser confusas e ininteligibles comparadas a una foto natural. Veremos esto a medida que lo implementemos nosotros mismos.</p>
<h2 id="implementando-visualizacion">Implementando visualización<a class="zola-anchor" href="#implementando-visualizacion" aria-label="Anchor link for: implementando-visualizacion">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Utilizando PyTorch, implementaremos visualización de clases para un modelo de clasificación preentrenado. Vamos a optimizar una imagen para que el modelo la clasifique en una clase de ImageNet de nuestra elección. Entonces, ¿qué clase elegiremos?</p>









    



    
    
    

    
        
    

<figure >
    <img src="https:&#x2F;&#x2F;sinono3.github.io&#x2F;processed_images&#x2F;hen.7c53653065aed7b3.jpg"  />
    <figcaption><p>Clase 8 de ImageNet: <em>gallina</em>. <a href="https://unsplash.com/photos/brown-and-red-he-n-G61iAuzI9NQ">Source.</a></p>
</figcaption>
</figure>
 
<p>¿Por qué gallinas? Porque <strong>todas</strong> tienen carúnculas rojas que son fácilmente reconocibles. Entonces, será más fácil saber si nuestra visualización funciona o no desde ya. (Si no vemos formas rojas, algo anda mal.)</p>
<blockquote>
<p><strong>En caso que quieras usar otra clase de ImageNet</strong>, <a href="https://github.com/pytorch/hub/blob/c7895df70c7767403e36f82786d6b611b7984557/imagenet_classes.txt">aquí está la lista de la cual puedes elegir</a>. Una vez que te decidiste, grabá el número de línea del mismo y restale 1 para obtener el índice de la clase. (Esto es porque los números de línea empiezan en 1, mientras que los tensores de PyTorch empiezan en 0)</p>
</blockquote>
<p>Vamos a visualizar features del modelo ResNet18. Obtuve resultados decentes con este modelo durante mi propia experimentación. Es posible obtener mejores visualizaciones con modelos más grandes como VGG19, pero al costo de la velocidad de la optimización. En este caso, prefiero feedback loops más rápidos a mejor calidad, porque estas nos permitirán experimentar con facilidad.</p>
<h3 id="caso-base-optimizar-el-input-como-optimizar-parametros-de-un-modelo">Caso base: Optimizar el input como optimizar parámetros de un modelo<a class="zola-anchor" href="#caso-base-optimizar-el-input-como-optimizar-parametros-de-un-modelo" aria-label="Anchor link for: caso-base-optimizar-el-input-como-optimizar-parametros-de-un-modelo">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Empezaremos el código importando matplotlib como nuestro backend para mostrar imágenes convenientemente. También definiremos una función <code>ptimg_to_mplimg</code> para convertir imágenes de un tensor de PyTorch a un array de numpy, disponibilizandolo así para visualización en matplotlib. Definiremos <code>show_img</code> para visualizar imágenes concisamente en una sola llamada de función.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>torch
</span><span style="color:#b48ead;">import </span><span>torchvision
</span><span style="color:#b48ead;">import </span><span>matplotlib.pyplot </span><span style="color:#b48ead;">as </span><span>plt
</span><span>
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">ptimg_to_mplimg</span><span>(</span><span style="color:#bf616a;">input</span><span>: torch.Tensor):
</span><span>    </span><span style="color:#b48ead;">with </span><span>torch.</span><span style="color:#bf616a;">no_grad</span><span>():
</span><span>        </span><span style="color:#b48ead;">return </span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">detach</span><span>().</span><span style="color:#bf616a;">squeeze</span><span>().</span><span style="color:#bf616a;">permute</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">2</span><span>, </span><span style="color:#d08770;">0</span><span>).</span><span style="color:#bf616a;">clamp</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#d08770;">1</span><span>).</span><span style="color:#bf616a;">numpy</span><span>()
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">show_img</span><span>(</span><span style="color:#bf616a;">input</span><span>: torch.Tensor, </span><span style="color:#bf616a;">title</span><span>: str):
</span><span>    plt.</span><span style="color:#bf616a;">imshow</span><span>(</span><span style="color:#bf616a;">ptimg_to_mplimg</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    plt.</span><span style="color:#bf616a;">title</span><span>(title)
</span><span>    </span><span style="color:#65737e;"># Setting `block=False` and calling `plt.pause` allow us to display the progress in real time
</span><span>    plt.</span><span style="color:#bf616a;">show</span><span>(</span><span style="color:#bf616a;">block</span><span>=</span><span style="color:#d08770;">False</span><span>)
</span><span>    plt.</span><span style="color:#bf616a;">pause</span><span>(</span><span style="color:#d08770;">0.1</span><span>)
</span></code></pre>
<p>Con el boilerplate ya escrito, vamos a implementar la forma más simple y obvia de hacer visualización de clases. Vamos a guiarnos de como solemos optimizar redes neuronales: usando un optimizador built-in de PyTorch que ajusta parámetros para minimizar nuestro loss function (función de pérdida). Aquí, intentaremos lo mismo, pero en vez de optimizar los parámetros de un modelo, optimizaremos el input (nuestra imagen). &quot;¿Cuál será nuestro loss function?&quot;, te estarás preguntando. Responderemos aquello después.</p>
<p>Descargemos el modelo preentrenado.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span>model = torch.hub.</span><span style="color:#bf616a;">load</span><span>(&quot;</span><span style="color:#a3be8c;">pytorch/vision:v0.10.0</span><span>&quot;, &quot;</span><span style="color:#a3be8c;">resnet18</span><span>&quot;, </span><span style="color:#bf616a;">weights</span><span>=&quot;</span><span style="color:#a3be8c;">ResNet18_Weights.IMAGENET1K_V1</span><span>&quot;)
</span><span style="color:#65737e;"># Ponemos el modelo en modo de evaluación
</span><span style="color:#65737e;"># Desactiva capas dropout y de batch normalization, las cuales necesitamos ahora
</span><span>model.</span><span style="color:#bf616a;">eval</span><span>()
</span></code></pre>
<p>Necesitamos definir la imagen que será nuestro punto de inicio para la optimización. Usaremos valores random de 0 a 1. Es importante notar que podemos iniciar con cualquier imagen.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">input </span><span>= torch.</span><span style="color:#bf616a;">rand</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">3</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#bf616a;">requires_grad</span><span>=</span><span style="color:#d08770;">True</span><span>)
</span></code></pre>
<p>Declaramos nuestra clase inicial como &quot;gallina&quot;.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">TARGET_CLASS </span><span>= </span><span style="color:#d08770;">8 </span><span style="color:#65737e;"># kokoroko!
</span></code></pre>
<p>Hmm, ¿qué optimizador deberíamos usar? ¿Por qué no el clásico SGD? (stochastic gradient descent)</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">LR </span><span>= </span><span style="color:#d08770;">0.5
</span><span>optimizer = torch.optim.</span><span style="color:#bf616a;">SGD</span><span>([</span><span style="color:#96b5b4;">input</span><span>], </span><span style="color:#bf616a;">lr</span><span>=</span><span style="color:#bf616a;">LR</span><span>)
</span></code></pre>
<p>Crearemos una función que realiza un paso de optimización para que nuestro código esté &quot;ordenado&quot;.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#96b5b4;">input</span><span>)
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>
<p>Acá hicimos algo importante: <strong>definimos nuestro loss function como el negativo de la activación de la neurona de output correspondiente a nuestra clase target.</strong> Queremoos que la activación de esta neurona sea lo mayor posible. Nuestro optimizador intenta <em>minimizar</em> el loss function, por lo tanto, si definimos el loss function como el negativo de la activación de nuestra neurona target, el optimizador intentará maximizar la activación de la neurona target.</p>
<p>Ahora, sencillamente necesitamos llamar la función &quot;step&quot; en un bucle, mostrando nuestra imagen cada tanto para ver el progreso.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">STEPS </span><span>= </span><span style="color:#d08770;">200
</span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#bf616a;">STEPS</span><span>):
</span><span>    </span><span style="color:#bf616a;">step</span><span>()
</span><span>
</span><span>    </span><span style="color:#65737e;"># Mostrar la imagen cada 10 pasos
</span><span>    </span><span style="color:#b48ead;">if </span><span>i % </span><span style="color:#d08770;">10 </span><span>== </span><span style="color:#d08770;">0</span><span>:
</span><span>        </span><span style="color:#96b5b4;">print</span><span>(</span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Step </span><span>{i}</span><span style="color:#a3be8c;">/</span><span>{</span><span style="color:#bf616a;">STEPS</span><span>}&quot;)
</span><span>        </span><span style="color:#bf616a;">show_img</span><span>(</span><span style="color:#96b5b4;">input</span><span>, </span><span style="color:#b48ead;">f</span><span>&quot;</span><span style="color:#a3be8c;">Visualization of class </span><span>{</span><span style="color:#bf616a;">TARGET_CLASS</span><span>}&quot;)
</span></code></pre>
<p>Okay! Completamos nuestra primera versión. Veamos qué tal le va.</p>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app0.mp4 type="video/mp4"></video>
    
</figure>

<p>Hmm, no parece tanto una gallina. ¿Qué podemos hacer para resolver esto?</p>
<h3 id="mejora-1-cambiar-la-imagen-inicial">Mejora 1: Cambiar la imagen inicial<a class="zola-anchor" href="#mejora-1-cambiar-la-imagen-inicial" aria-label="Anchor link for: mejora-1-cambiar-la-imagen-inicial">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Iniciar con valores random de 0 a 1 puede estar causando que el optimizador tienda a valores extremos (fuera del rango RGB 0-1). No solo esto genera alto contraste en la imagen, pero si queremos deshacernos del ruido, iniciar con una imagen ya ruidosa capaz no sea la mejor opción. Vamos a intentar una imágen más uniformemente gris. Siendo preciso, usarémos exactamente el mismo ruido pero con una media de 0.5 y un rango de 0.49-0.51.</p>
<p>Vamos a reflejar estos cambios en el código.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#96b5b4;">input </span><span>= (torch.</span><span style="color:#bf616a;">rand</span><span>(</span><span style="color:#d08770;">1</span><span>, </span><span style="color:#d08770;">3</span><span>, </span><span style="color:#d08770;">299</span><span>, </span><span style="color:#d08770;">299</span><span>) - </span><span style="color:#d08770;">0.5</span><span>) * </span><span style="color:#d08770;">0.01 </span><span>+ </span><span style="color:#d08770;">0.5
</span><span style="color:#96b5b4;">input</span><span>.requires_grad = </span><span style="color:#d08770;">True
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app1.mp4 type="video/mp4"></video>
    
</figure>

<p>Mucho mejor! Si uno entrecierra los ojos, las cabezas rojas de las gallinas resaltan, mientras que en el resto de la imagen, emergen patrones similares a plumas.</p>
<h3 id="mejora-2-robustez-transformacional">Mejora 2: Robustez transformacional<a class="zola-anchor" href="#mejora-2-robustez-transformacional" aria-label="Anchor link for: mejora-2-robustez-transformacional">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Hemos estado calculando los gradientes basado en la misma imagen, a la misma escala, rotación y translación para cada paso.
Esto significa que nuestro código optimiza para que la imagen sea clasificada como &quot;gallina&quot; solo desde <em>un</em> punto de vista.
Si rotamos la imagen, nada nos asegura que seguirá siendo clasificada como &quot;gallina&quot;.
Nuestra imagen no es <em>transformacionalmente robusta</em>.</p>
<p>Si queremos que el modelo reconozca nuestro target class en una imagen despues de ser escalada o rotada, debemos incluir aquel proposito en nuestra optimización.</p>
<blockquote>
<p><strong>Por qué esto ayudaría con el problema del ruido?</strong></p>
</blockquote>
<p>No me está tan claro el porqué, sinceramente.
Hablando <em>vagamente</em>, introducir transformaciones estocásticas parece prevenir que el optimizador se mantenga pegado a algún patron ruidoso.
Para una explicación mucho mejor, podés revisar <a href="https://distill.pub/2017/feature-visualization">la sección de <em>Transformational robustness</em> del papel de <em>Feature Visualization</em> en Distill.</a></p>
<p>De todos modos, la implementación conlleva aplicar transformaciones aleatorias al input antes de la optimización.</p>
<p>Antes de nuestro step function, tenemos que definir qué transformaciones vamos a aplicar nuestra imagen antes de pasarla a nuestro modelo.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">from </span><span>torchvision </span><span style="color:#b48ead;">import </span><span>transforms </span><span style="color:#b48ead;">as </span><span>T
</span><span>transforms = T.</span><span style="color:#bf616a;">Compose</span><span>([
</span><span>    </span><span style="color:#65737e;"># Aplica cambios minúsculos en brillo, contraste, saturación y tono
</span><span>    T.</span><span style="color:#bf616a;">ColorJitter</span><span>(</span><span style="color:#bf616a;">brightness</span><span>=(</span><span style="color:#d08770;">0.98</span><span>, </span><span style="color:#d08770;">1.02</span><span>), </span><span style="color:#bf616a;">contrast</span><span>=(</span><span style="color:#d08770;">0.99</span><span>, </span><span style="color:#d08770;">1.01</span><span>), </span><span style="color:#bf616a;">saturation</span><span>=(</span><span style="color:#d08770;">0.98</span><span>, </span><span style="color:#d08770;">1.02</span><span>), </span><span style="color:#bf616a;">hue</span><span>=(-</span><span style="color:#d08770;">0.01</span><span>, </span><span style="color:#d08770;">0.01</span><span>)),
</span><span>    </span><span style="color:#65737e;"># Rota la imagen por 15 grados al sentido del reloj o contra el sentido del relo. También aplicamos agrandamos o achicamos un poquito la imagen.
</span><span>    T.</span><span style="color:#bf616a;">RandomAffine</span><span>(</span><span style="color:#bf616a;">degrees</span><span>=(-</span><span style="color:#d08770;">15.0</span><span>, </span><span style="color:#d08770;">15.0</span><span>), </span><span style="color:#bf616a;">scale</span><span>=(</span><span style="color:#d08770;">0.96</span><span>,</span><span style="color:#d08770;">1.04</span><span>), </span><span style="color:#bf616a;">fill</span><span>=</span><span style="color:#d08770;">0.5</span><span>),
</span><span>])
</span></code></pre>
<p>En nuestro step function, debemos modificar nuestro llamado al forward-pass.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))	</span><span style="color:#65737e;"># &lt;-- esta línea
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app2.mp4 type="video/mp4"></video>
    <figcaption><p>Resultado después de optimizar para robustez transformacional.</p>
</figcaption>
</figure>

<blockquote>
<p><strong>Note on gradient propagation:</strong> While I was originally implementing transformational robustness, I misunderstood its concept and <em>actually transformed the visualization</em>, instead of <em>just doing gradient propagation on</em> the transformed image. The difference is in the step function:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#65737e;"># Caso 1: VERDADERAMENTE TRANSFORMAR LA IMAGEN (permanentemente) (no probar en casa)
</span><span style="color:#96b5b4;">input </span><span>= </span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">detach</span><span>())
</span><span>output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#96b5b4;">input</span><span>)
</span><span style="color:#65737e;"># Caso 2: SOLO HACIENDO PROPAGACIÓN DE GRADIENTES EN LA IMAGEN TRANSFORMADA
</span><span>output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span></code></pre>
</blockquote>
<h3 id="mejora-3-implementar-regularizacion-l2">Mejora 3: Implementar regularización L2<a class="zola-anchor" href="#mejora-3-implementar-regularizacion-l2" aria-label="Anchor link for: mejora-3-implementar-regularizacion-l2">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Regularización L2. Hmm, qué? Sencillamente significa agregar el cuadrado de los parámetros que estamos optimizando a nuestro loss function. Precisamente, agregaremos la suma de los cuadrados de nuestros parámetros multiplicados por un coeficiente, normalmente llamado λ (lambda). En nuestro caso, los parámetros son los valores de color para cada pixel.</p>
<p>Esto básicamente penaliza valores de color lejanos a 0. En nuestro caso, no queremos valores que estén muy lejos de 0.5, el &quot;punto medio&quot; entre 0.0 y 1.0, el rango de valores de color. Esto permite a nuestra optimización tener un balance entre maximizar nuestra activación target (ya sea clase, neurona, capa, lo que sea) y hacer que nuestra imagen yazca en el rango de colores válidos. Nos ayudará a deshacernos de valores extremos en los canales rojo, verde o azul.</p>
<p>Implementar la regularización es bastante fácil. Sólo debemos definir λ y cambiar una linea en la definición de nuestro loss function.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#bf616a;">LR </span><span>= </span><span style="color:#d08770;">1.5 </span><span style="color:#65737e;"># También subiremos el learning rate
</span><span>L2_lambda = </span><span style="color:#d08770;">0.05 </span><span style="color:#65737e;"># definimos λ
</span></code></pre>
<p>En nuestro step function:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>():
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>() + L2_lambda * ((</span><span style="color:#96b5b4;">input </span><span>- </span><span style="color:#d08770;">0.5</span><span>) ** </span><span style="color:#d08770;">2</span><span>).</span><span style="color:#bf616a;">sum</span><span>()     </span><span style="color:#65737e;"># &lt;-- esta línea
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span></code></pre>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app3.mp4 type="video/mp4"></video>
    
</figure>

<h3 id="mejora-4-desenfocar-la-imagen-cada-tantos-pasos">Mejora 4: Desenfocar la imagen cada tantos pasos<a class="zola-anchor" href="#mejora-4-desenfocar-la-imagen-cada-tantos-pasos" aria-label="Anchor link for: mejora-4-desenfocar-la-imagen-cada-tantos-pasos">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Ahora para una técnica final, introduciré una técnica medio obvia para deshacernos del ruido que puede funcionar sorprendentemente bien: sencillamente aplicar Gaussian blur a la imagen. Claro, si hacemos esto repetidamente solo tendremos una imagen borrosa. Sin embargo, si lo hacemos ocasionalmente podemos tener buenos resultados.</p>
<p>Agregaremos un parámetro <code>i</code> a nuestro step function para saber en qué paso estamos.
Arbitrariamente, he puesto que en la step function la imagen se desenfoque cada 10 pasos.</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">step</span><span>(</span><span style="color:#bf616a;">i</span><span>: int):
</span><span>    optimizer.</span><span style="color:#bf616a;">zero_grad</span><span>()
</span><span>    output = </span><span style="color:#bf616a;">model</span><span>(</span><span style="color:#bf616a;">transforms</span><span>(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>    loss = -output[:, </span><span style="color:#bf616a;">TARGET_CLASS</span><span>].</span><span style="color:#bf616a;">mean</span><span>() + L2_lambda * ((</span><span style="color:#96b5b4;">input </span><span>- </span><span style="color:#d08770;">0.5</span><span>) ** </span><span style="color:#d08770;">2</span><span>).</span><span style="color:#bf616a;">sum</span><span>()
</span><span>    loss.</span><span style="color:#bf616a;">backward</span><span>()
</span><span>    optimizer.</span><span style="color:#bf616a;">step</span><span>()
</span><span>
</span><span>    </span><span style="color:#b48ead;">if </span><span>i % </span><span style="color:#d08770;">10 </span><span>== </span><span style="color:#d08770;">0</span><span>:
</span><span>        </span><span style="color:#b48ead;">with </span><span>torch.</span><span style="color:#bf616a;">no_grad</span><span>():
</span><span>            </span><span style="color:#96b5b4;">input</span><span>.</span><span style="color:#bf616a;">copy_</span><span>((T.</span><span style="color:#bf616a;">GaussianBlur</span><span>(</span><span style="color:#d08770;">5</span><span>, </span><span style="color:#d08770;">1.5</span><span>))(</span><span style="color:#96b5b4;">input</span><span>))
</span><span>
</span><span>
</span><span style="color:#bf616a;">STEPS </span><span>= </span><span style="color:#d08770;">200
</span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#bf616a;">STEPS</span><span>):
</span><span>    </span><span style="color:#bf616a;">step</span><span>(i)
</span><span>    </span><span style="color:#65737e;"># ... el resto del código...
</span></code></pre>
<p>El desenfoque coincidirá exactamente con nuestro display de la imagen, tan solo para visualizar el efecto (el efecto será más obvio).
Si uno no quiere esto, puede cambiar la condición de desenfoque a <code>i % 10 == 1</code>.
Esto hará que la imagen se desenfoque <em>exactamente el paso después</em> de mostrar la imagen, en vez de antes.
de mostrar la imagen, en vez de antes.</p>


<figure >
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;app4.mp4 type="video/mp4"></video>
    
</figure>

<h2 id="probando-nuestra-visualizacion-con-otras-clases">Probando nuestra visualización con otras clases<a class="zola-anchor" href="#probando-nuestra-visualizacion-con-otras-clases" aria-label="Anchor link for: probando-nuestra-visualizacion-con-otras-clases">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Ahora que tenemos algo que funciona, probemoslo con varias clases.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;conclusion.mp4 type="video/mp4"></video>
    <figcaption><p>De izquierda-arriba a derecha-abajo: gallina (8), pelícano (144), leopardo (288), martillo (587), iPod (605), ranura (800), estofado de carne (964), buceador (983).</p>
</figcaption>
</figure>

<h3 id="experimentacion-extrana-zoom-eterno">Experimentación extraña (zoom eterno)<a class="zola-anchor" href="#experimentacion-extrana-zoom-eterno" aria-label="Anchor link for: experimentacion-extrana-zoom-eterno">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h3>
<p>Inicialmente al implementar robustez transformacional, malentendí el concepto y <em>transforme la visualización</em>, en vez de <em>solo hacer propagación de gradiente</em> en la imagen transformada. Durante estos tiempos difíciles, experimenté con continuamente aumentar el tamaño de la imagen. El resultado en las visualizaciones es un zoom-in eterno, pero la atmosfera es una de sumergirse en mundos alienígenas sin fin.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;alien3.mp4 type="video/mp4"></video>
    <figcaption><p>Izquierda: 'gallina' (8). Derecha: 'petirrojo' (15)</p>
</figcaption>
</figure>

<p>También implementé una especie de regularización L1 donde sencillamente el valor de los colores multiplicados por el coeficiente son agregados al loss function. Esto da resultados muy interesantes, pero no son tan representantivos de los conceptos.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;alien.mp4 type="video/mp4"></video>
    <figcaption><p>Izquierda: 'gallina' (8). Derecha: 'petirrojo' (15)</p>
</figcaption>
</figure>

<p>Me atrevo a decir que estas visualizaciones animadas, en vez de ser 100% inútiles, nos dan una perspectiva diferente para entender qué visualizan.</p>


<figure class="extended-figure">
    <video  controls  muted loop width="100%"><source src=&#x2F;blog&#x2F;feature&#x2F;barbellwine.mp4 type="video/mp4"></video>
    <figcaption><p>Izquierda: Mundo de las barras. Derecha: Mundo del vino.</p>
</figcaption>
</figure>

<h2 id="limitations">Limitations<a class="zola-anchor" href="#limitations" aria-label="Anchor link for: limitations">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Nuestro método actual definivamente tiene limitaciones:</p>
<ul>
<li>La regularización L2 inherentemente reduce el contraste de colores ya que desfavora valores que se alejan de [0.5, 0.5, 0.5], A pesar de que en el mundo real, sí hay muchas cosas con colores muy extremos.</li>
<li>ResNet18 es un modelo relativamente pequeño, así que sus visualizaciones pueden no ser de tan alta calidad como esas de modelos más grandes.</li>
<li>ResNet18 seems to have a bias towards the color green. This is a common feature among many of the models I've tested. <em>My theory</em>: since many of the classes and thus training data are animal-related, grass is a common denominator in the background of these pictures. Therefore, the model sees a lot of green during its training, which generates a bias.</li>
<li>ResNet18 parece tener un bias hacia el color verde. Esto fue un factor común en muchos de los modelos que he probado. <em>My teoría</em>: ya que muchas de las clases y por ende datos de entrenamiento, son relacionados a animales, el pasto es común en todas las imágenes. Entonces, el modelo parece ver mucho verde durante su entrenamiento, que genera un bias.</li>
<li><a href="https://en.wikipedia.org/wiki/Bilateral_filter">Hay alternativas mejores a Gaussian blur para nuestro propósito.</a></li>
</ul>
<p>Leer <a href="https://distill.pub/2017/feature-visualization/">este artículo te puede dar una idea de lo loco que puede verse la visualización de features cuando está bien implementada.</a>.</p>
<h2 id="conclusion">Conclusion<a class="zola-anchor" href="#conclusion" aria-label="Anchor link for: conclusion">
    <svg class="zola-anchor-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true">
        <path fill-rule="evenodd"
            d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z">
        </path>
    </svg>
</a>
</h2>
<p>Esto es suficiente para el post. Espero que la interpretabilidad te haya parecido divertida e interesante.</p>
<p>Y hey, si es así, no pares aquí! Apenas tuvimos un vistazo superficial de lo que realmente se trata el campo. Hay un montón de recursos para seguir investigando.Lastimosamente, casi no existe contenido del tema en español. (Por eso hice este post, lo cuál fue difícil porque gran parte de la terminología no existe en español). Si uno quiere empezar en el campo, es clave aprender inglés. Uno podrá llegar mucho más lejos con la barrera del idioma rota. De todos modos, les dejo los links a los artículos en inglés:</p>
<ul>
<li><a href="https://distill.pub/2020/circuits/">Distill's Circuits Thread</a>: Artículos excelentes y accesibles de interpretabilidad. Contiene <a href="https://distill.pub/2017/feature-visualization">un paper específicamente relacionado a la visualización de features</a></li>
<li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a>: Artículos de interpretabilidad de transformers y modelos de lenguaje.</li>
<li><a href="https://www.arena.education/">ARENA Course</a>: Práctica práctica para habilidades de alineación de IA y interpretabilidad.</li>
</ul>

    </div>
    

    
</article>


    <footer>
        <p>
            
<span class="icon-text">
  

  

  
  <a href="https://github.com/Sinono3" aria-label="Open Sinono3 GitHub" target="_blank">
    <span class="icon is-large">
      <i class="fab fa-github fa-lg" aria-hidden="true" title="GitHub"></i>
    </span>
  </a>
  

  
  <a href="https://www.linkedin.com/in/aldo-acevedo-9a38a9289" aria-label="Open aldo-acevedo-9a38a9289 LinkedIn" target="_blank">
    <span class="icon is-large">
      <i class="fab fa-linkedin fa-lg" aria-hidden="true" title="LinkedIn"></i>
    </span>
  </a>
  

  

  

  

  
  <a href="mailto:aldoacevedo@protonmail.com" aria-label="Email aldoacevedo@protonmail.com" target="_blank">
    <span class="icon is-large">
      <i class="far fa-envelope fa-lg" aria-hidden="true" title="email"></i>
    </span>
  </a>
  

  

  
</span>

        </p>
        <p>
            
            
        </p>
    </footer>
</div>
</body>
</html>
