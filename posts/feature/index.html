<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Aldo Acevedo</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <link href="/styles.css" rel="stylesheet">
		<link
			href="https://unpkg.com/prismjs@1.20.0/themes/prism-coy.css"
			rel="stylesheet"
		/>
  </head>

  <body>
    <header>
    </header>
    <main>
      
<h1>A Brief, Practical Introduction to Feature Visualization in PyTorch</h1>

<figure >
          <video autoplay muted loop><source src=castle_sped.mp4 type="video/mp4"></video>
          <figcaption>
        Visualization of castle (483)
      </figcaption>
      </figure>
<p>Growing more capable by the second, AI is being adopted by both industries and governments as a mainstream technology.
AI allows automation of knowledge work. With enough data, compute, and algorithmic improvements, AI can replace engineering,
research, and administrative jobs.</p>
<p>This could be great or detrimental, depending on how effectively these technologies achieve our goals.
There's a bunch of reasons as to why this is, among them:</p>
<ol>
<li>We don't understand how deep learning models <em>actually</em> work.</li>
<li>We don't know how to make deep learning models do <em>precisely</em> what we want.</li>
<li>We don't even know what we want. (Or rather, how to explicitly state what we want)</li>
<li>We don't know which regulations reduce AI misuse most effectively.</li>
<li><em>...many more</em></li>
</ol>
<p>While all these problems are critical, this post focuses on problem 1: <strong>interpretability</strong>. I'm not an expert in this field, but I have <em>just</em> enough hands-on experience to lend a tiny hand to those looking to get started.</p>
<!-- TODO: You can check out these resources to answer the other questions... -->
<blockquote>
<p><strong>Heads-up</strong>: I assume some understanding of <a href="https://www.youtube.com/watch?v=aircAruvnKk">feed-forward neural networks (MLP)</a> and <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w">gradient descent</a>.</p>
</blockquote>
<h2>What does interpretability solve?</h2>
<p>Let's go back to the problem statement:</p>
<blockquote>
<p>We don't understand how deep learning models <em>actually</em> work.</p>
</blockquote>
<p>What do I mean by &quot;how they <em>actually</em> work&quot;?</p>
<p>For decades, we have built different deep learning architectures such as multi-layer perceptrons; convolutional, residual and recurrent neural networks; LSTMs; transformers; and many more. During training, these machines encode patterns and algorithms inside their parameters (weights, biases, etc). We understand how the process works, but not the specific patterns and algorithms that emerge.</p>
<p>Once trained, <strong>models are black boxes</strong>: while we provide inputs and get mostly correct outputs, we don't understand how the model did so at a neuron level. We know that neurons are connected, but <em>how</em> are they connected to achieve the goal?</p>
<p>I'll present you with an example of what I mean. Consider <a href="https://unsplash.com/photos/black-and-white-cat-lying-on-brown-bamboo-chair-inside-room-gKXKBY-C-Dk">this cat</a>.</p>
<figure >
          <img src="cat.jpg"/>
          <figcaption>
        [*(Consider them.)*](https://aisafety.dance/)
      </figcaption>
      </figure> 
<p>If fed to an image classification model such as ResNet18, it would be classified as &quot;Egyptian cat,&quot; which wouldn't be far from the truth. There's no single &quot;cat&quot; category in the model's output categories, so it would be impossible for the model to simply answer &quot;cat.&quot; So, practically, it's correct!</p>
<figure class="extended">
    <img src="ResNet18.excalidraw.svg" />
</figure>
<p>But how did the model come to that conclusion? Let's do a reverse analysis.</p>
<ul>
<li>Since the ImageNet class &quot;Egyptian cat&quot; has an index of 285, we know that in the fully connected last layer of the model (<code>fc</code>), neuron 285 has the greatest activation among the neurons in that layer (which are 1000 in total). This is because the last operation (<code>argmax</code>) returns the index of the previous layer's neuron with the greatest activation.</li>
<li>Neuron 285 in <code>fc</code> was activated because of some neuron activations in the previous layer (<code>avgpool</code>).</li>
<li>Neurons in <code>avgpool</code> that contributed to neuron 285 in <code>fc</code> come from the results of a convolutional layer.</li>
<li>This convolutional layer calculated its output using another convolutional layer's output.</li>
<li>And so on... until we get to the first layer, which is connected directly to the input image (cute cat pic).</li>
</ul>
<p>From this analysis, multiple questions pop up:</p>
<ol>
<li><strong>Circuit identification:</strong> Which neurons in <code>avgpool</code>, when activated, cause neuron 285 of layer <code>fc</code> to activate? And in the previous layer? What complex circuit of neurons has formed across the network's layers to conclude this image corresponds to an &quot;Egyptian cat&quot;?</li>
<li><strong>Visualization:</strong> What do these neurons firing represent? Do they correspond to concepts, shapes, forms, or objects? Can we see an image of what a single neuron represents? What about a set of neurons?</li>
<li><strong>Attribution</strong>: What parts of the input image contributed to the model outputting &quot;Egyptian cat&quot;? Which ones didn't? What parts of the input image contribute to a particular neuron firing? What images make a neuron fire?</li>
</ol>
<p>At the surface level, interpretability tries to answer these kinds of questions.</p>
<p>Today, we're going to have our try at <strong>visualization</strong>.</p>
<h2>Defining visualization</h2>
<p>In the general case of any network, we can define <em>feature visualization</em> as generating an input that maximizes the activation of a part of the network: an output neuron, a hidden-layer neuron, a set of neurons, or an entire layer.</p>
<p>In the case of image classification models, feature <em>visualization</em> quite literally refers to generating an image. Let's say we do <em>class</em> visualization, where we optimize an image so the model <em>overwhelmingly</em> classifies it in a particular class (meaning the neuron corresponding to that class in the last layer will be significantly activated, more than all the other output neurons.)</p>
<p>In a perfect world, if we were to visualize class 285 on ResNet18, we would get an image of a cute kitten. In reality, though, feature visualizations can be confusing and unintelligible compared to a natural picture. We'll see this as we try to implement it ourselves.</p>
<h2>Implementing visualization</h2>
<p>Using PyTorch, let's implement class visualization for a pre-trained image classification model. We're going to choose a specific ImageNet class and optimize an image so the model classifies it in the specified class. So, which class are we choosing?</p>
<figure >
          <img src="hen.jpg"/>
          <figcaption>
        ImageNet class 8: *hen*. [Source.](https://unsplash.com/photos/brown-and-red-he-n-G61iAuzI9NQ)
      </figcaption>
      </figure> 
<p>Why chickens? Because <strong>all</strong> of them they easily recognizable red caruncles (look it up). Thus, it will be easier to see if our visualization works at all from the get-go.</p>
<blockquote>
<p><strong>In case you want to use another ImageNet class</strong>, <a href="https://github.com/pytorch/hub/blob/c7895df70c7767403e36f82786d6b611b7984557/imagenet_classes.txt">here's the list you can choose from</a>. Once you did, record the line number of the label and subtract 1 to get the output neuron or class index. (This is because line numbers start at 1, while PyTorch tensors indexes do at 0)</p>
</blockquote>
<p>We're going to visualize the ResNet18 model. I obtained &quot;good&quot; results with this model during my own experimentation. Better visualizations can be obtained with larger models such as VGG19, but at the cost of optimization speed. In this case, I'm trading off better quality for quicker feedback loops, allowing easier experimentation.</p>
<h3>Base case: Optimize the input like we're optimizing model parameters</h3>
<p>We will begin writing code by importing matplotlib as our backend to display images conveniently. We'll also define a function <code>ptimg_to_mplimg</code> to convert the images from a PyTorch tensor to a numpy array, suitable for display in matplotlib. We define <code>show_img</code> to be able to display images concisely in a single function call.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">def</span> <span class="token function">ptimg_to_mplimg</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">show_img</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> title<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>ptimg_to_mplimg<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span>title<span class="token punctuation">)</span>
    <span class="token comment"># Setting `block=False` and calling `plt.pause` allow us to display the progress in real time</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span>block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>pause<span class="token punctuation">(</span><span class="token number">0.1</span><span class="token punctuation">)</span></code></pre>
<p>With the boilerplate out of the way, let's go ahead and implement the simplest, most obvious way to do class visualization. We'll refer to how we usually train neural networks: using a built-in PyTorch optimizer which adjusts parameters to minimize the loss function. Here, we will try to do the same, but instead of optimizing the model's parameters, we will optimize the input. &quot;What will be our loss function?&quot; you may ask. We'll answer that later.</p>
<p>Let's download the pre-trained model:</p>
<pre class="language-python"><code class="language-python">model <span class="token operator">=</span> torch<span class="token punctuation">.</span>hub<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"pytorch/vision:v0.10.0"</span><span class="token punctuation">,</span> <span class="token string">"resnet18"</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token string">"ResNet18_Weights.IMAGENET1K_V1"</span><span class="token punctuation">)</span>
<span class="token comment"># Set the model to evaluation mode: </span>
<span class="token comment"># Disables dropout and batch normalization layers, which we don't need right now</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<p>We need to define our initial image, which will be the starting point for the optimization. We'll use uniformly random values ranging from 0 to 1. Do notice that we can use any image as the starting point.</p>
<pre class="language-python"><code class="language-python"><span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></code></pre>
<p>We declare our target class to be &quot;hen.&quot;</p>
<pre class="language-python"><code class="language-python">TARGET_CLASS <span class="token operator">=</span> <span class="token number">8</span> <span class="token comment"># cluck cluck, buck buck bugawk</span></code></pre>
<p>Hmm, what optimizer should we use? Why not plain ol' SGD? (stochastic gradient descent)</p>
<pre class="language-python"><code class="language-python">LR <span class="token operator">=</span> <span class="token number">0.5</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">input</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>LR<span class="token punctuation">)</span></code></pre>
<p>We'll create a function that performs a single optimization step to organize our code neatly.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> TARGET_CLASS<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<p>Here, we have done something important: <strong>We defined our loss function as the negative of the activation of the output neuron corresponding to the target class</strong>. We want this neuron's activation to be as great as possible. Our optimizer tries to <em>minimize</em> the loss function, thus, if we set the loss function to be the negative of the target neuron's activation, the optimizer will try to maximize the target neuron's activation.</p>
<p>Now, we simply need to call the step function in a loop, displaying our image every few steps to see the progress.</p>
<pre class="language-python"><code class="language-python">STEPS <span class="token operator">=</span> <span class="token number">200</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span><span class="token punctuation">:</span>
    step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Show image every 10 steps</span>
    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Step </span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{</span>STEPS<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        show_img<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"Visualization of class </span><span class="token interpolation"><span class="token punctuation">{</span>TARGET_CLASS<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span></code></pre>
<p>Okay! We've completed our first version. Let's see how it does.</p>
<figure >
          <video autoplay muted loop><source src=app0.mp4 type="video/mp4"></video>
          <figcaption>
        Visualization of fowl (483)
      </figcaption>
      </figure>
<p>Hmm. That doesn't quite look like a fowl. What can we do to improve this?</p>
<h3>Improvement 1: Change the initial image</h3>
<p>Starting with random values from 0 to 1 may cause the optimizer to tend to extreme values (outside the 0-1 RGB range). Not only do these values generate high contrast in the image, but if we want to get rid of the noise, starting with an already noisy image may not be the best option. Let's try a more uniformly gray initial image. To be more precise, the same random noise, but with a mean of 0.5 and a range of 0.49-0.51.</p>
<p>Let's reflect these changes in our code:</p>
<pre class="language-python"><code class="language-python"><span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">,</span> <span class="token number">299</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.01</span> <span class="token operator">+</span> <span class="token number">0.5</span>
<span class="token builtin">input</span><span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span></code></pre>
<figure >
          <video controls muted loop><source src=app1.mp4 type="video/mp4"></video>
<p></figure></p>
<p>That's a lot better! If one squints, the red caruncles of the chickens pop out, while in the rest of the image, feather-like patterns start to emerge.</p>
<h3>Improvement 2: Enforcing transformational robustness</h3>
<p>We've been calculating the gradients based on the same image at the same scale, rotation, and translation for every step.
This means our code optimizes for the image to be classified as &quot;hen&quot; only from one point of view.
If we rotate the image, it's not certain that it will still be classified as &quot;hen.&quot;
Our image is not <em>transformationally robust</em>.</p>
<p>If we want the model to recognize the target class in our image after being scaled or rotated, we must optimize the image to do so.</p>
<blockquote>
<p><strong>Why would this help with the noise problem?</strong></p>
</blockquote>
<p>I don't have it quite clear, actually.
Speaking <em>very vaguely</em>, introducing stochastic jittering, scale, and rotation seems to prevent the optimizer from sticking to a certain noisy pattern.
For a much better explanation, you can check <a href="https://distill.pub/2017/feature-visualization">the <em>Transformational robustness</em> section of the <em>Feature Visualization</em> paper on Distill.</a></p>
<p>Anyways, the implementation involves applying random transformations on the input before the optimization.</p>
<p>Before our step function, we must define what transforms we'll apply to our image before passing it to our model.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms <span class="token keyword">as</span> T
transforms <span class="token operator">=</span> T<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token comment"># Applies minor changes in brightness, contrast, saturation, and hue</span>
    T<span class="token punctuation">.</span>ColorJitter<span class="token punctuation">(</span>brightness<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">1.02</span><span class="token punctuation">)</span><span class="token punctuation">,</span> contrast<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.99</span><span class="token punctuation">,</span> <span class="token number">1.01</span><span class="token punctuation">)</span><span class="token punctuation">,</span> saturation<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.98</span><span class="token punctuation">,</span> <span class="token number">1.02</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hue<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># Rotates the image by 15deg clockwise or counter-clockwise. We also apply a bit of random zoom in and out.</span>
    T<span class="token punctuation">.</span>RandomAffine<span class="token punctuation">(</span>degrees<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">15.0</span><span class="token punctuation">,</span> <span class="token number">15.0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.96</span><span class="token punctuation">,</span><span class="token number">1.04</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fill<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span></code></pre>
<p>On our step function, we must modify the forward-pass call.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>transforms<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>	<span class="token comment"># &lt;-- this line right here</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> TARGET_CLASS<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<figure >
          <video controls muted loop><source src=app2.mp4 type="video/mp4"></video>
          <figcaption>
        Result after optimizing for transformational robustness.
      </figcaption>
      </figure>
<blockquote>
<p><strong>Note on gradient propagation:</strong> While I was originally implementing transformational robustness, I misunderstood its concept and <em>actually transformed the visualization</em>, instead of <em>just doing gradient propagation on</em> the transformed image. The difference is in the step function:</p>
<pre class="language-python"><code class="language-python"><span class="token comment"># Case 1: ACTUALLY TRANSFORMING THE VISUALIZATION (don't try at home)</span>
<span class="token builtin">input</span> <span class="token operator">=</span> transforms<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
<span class="token comment"># Case 2: JUST DOING GRADIENT PROPAGATION ON THE TRANSFORMED IMAGE</span>
output <span class="token operator">=</span> model<span class="token punctuation">(</span>transforms<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
</blockquote>
<h3>Improvement 3: Implement L2 regularization</h3>
<p>L2 regularization. Hmm, what? It's simply adding the square of the parameters we're optimizing to our loss function. Actually, we add the sum of the squares of our parameters multiplied by a coefficient, usually called λ (lambda). In this case, the parameters are the color values for each pixel.</p>
<p>This basically penalizes color values that deviate significantly from 0. In our case, we don't want values that stray too far from 0.5, the &quot;middle&quot; point between 0.0 and 1.0, the range of color values. This allows our optimization to have a balance between maximizing our target activation (be it class, neuron, layer, whatever) and having our image be in a valid color range. This will get rid of values that are too extreme on the red, green, or blue channels.</p>
<p>Implementing it is quite easy. We only need to define λ and change a line in the loss function definition.</p>
<pre class="language-python"><code class="language-python">LR <span class="token operator">=</span> <span class="token number">1.5</span> <span class="token comment"># We'll also increase the learning rate.</span>
L2_lambda <span class="token operator">=</span> <span class="token number">0.05</span></code></pre>
<p>In our step function:</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>transforms<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> TARGET_CLASS<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> L2_lambda <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">input</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># &lt;-- this line</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre>
<figure >
          <video controls muted loop><source src=app3.mp4 type="video/mp4"></video>
<p></figure></p>
<h3>Improvement 4: Blur the image every few steps</h3>
<p>Now, for a final technique, I'll introduce a somewhat obvious technique to get rid of noise, which can work surprisingly well: simply applying Gaussian blur to the image. Of course, if we do this repeatedly we will only get a blurry image. If done occasionally, though, we can obtain good results.</p>
<p>We will add a parameter to our step function so we can know our step index.
Arbitrarily, I've set the step function to blur the image every 10 steps.</p>
<pre class="language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>i<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> model<span class="token punctuation">(</span>transforms<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> <span class="token operator">-</span>output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> TARGET_CLASS<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> L2_lambda <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">input</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token builtin">input</span><span class="token punctuation">.</span>copy_<span class="token punctuation">(</span><span class="token punctuation">(</span>T<span class="token punctuation">.</span>GaussianBlur<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">)</span>


STEPS <span class="token operator">=</span> <span class="token number">200</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>STEPS<span class="token punctuation">)</span><span class="token punctuation">:</span>
    step<span class="token punctuation">(</span>i<span class="token punctuation">)</span>
    <span class="token comment"># ... the rest of the code</span></code></pre>
<p>This will coincide exactly with our image display, so you can see the blur effect.
To prevent this coincidence you can change the blurring condition to <code>i % 10 == 1</code>.
This will make the blurring occur exactly after displaying the image, instead of before.</p>
<figure >
          <video controls muted loop><source src=app4.mp4 type="video/mp4"></video>
<p></figure></p>
<h2>Testing our visualization on various classes</h2>
<p>Now that we've got something working, let's try our model with lots of different classes:</p>
<figure class="extended">
          <video controls muted loop><source src=conclusion.mp4 type="video/mp4"></video>
          <figcaption>
        From top-left to bottom-right: hen (8), pelican (144), leopard (288), hammer (587), iPod (605), slot (800), potpie (964), scuba diver (983).
      </figcaption>
      </figure>
<h3>Weird experimentation (eternal zoom-in)</h3>
<p>While initially implementing transformational robustness, I misunderstood the concept and <em>actually transformed the visualization</em>, instead of <em>just doing gradient propagation on</em> the transformed image. During these trying times, I experimented with biasing the transformations to continually increase the scale of the image. The result is an eternal zoom-in effect, but the atmosphere is that of endlessly submerging yourself in alien worlds.</p>
<figure >
          <video autoplay muted loop><source src=alien3.mp4 type="video/mp4"></video>
          <figcaption>
        Left: 'hen' (8). Right: 'robin' (15)
      </figcaption>
      </figure>
<p>I also implemented a sort-of L1 regularization where simply the color values multiplied by a lambda coefficient are added to the loss function. This gives some very trippy results.</p>
<figure >
          <video autoplay muted loop><source src=alien.mp4 type="video/mp4"></video>
          <figcaption>
        Left: 'hen' (8). Right: 'robin' (15)
      </figcaption>
      </figure>
<p>I'd dare say that these moving visualizations give us a different perspective to understand what a feature represents.</p>
<figure >
          <video autoplay muted loop><source src=barbellwine.mp4 type="video/mp4"></video>
          <figcaption>
        Left: Barbell World. Right: Wine World.
      </figcaption>
      </figure>
<h2>Limitations</h2>
<p>There are definitely some limitations to our current approach:</p>
<ul>
<li>L2 regularization inherently reduces color contrast since we unfavor color values that stray far from [0.5, 0.5, 0.5].</li>
<li>ResNet18 is a relatively small model, so its visualizations may not be as high quality as those from larger models.</li>
<li>ResNet18 seems to have a bias towards the color green. This is a common feature among many of the models I've tested. <em>My theory</em>: since many of the classes and thus training data are animal-related, grass is a common denominator in the background of these pictures. Therefore, the model sees a lot of green during its training, which generates a bias.</li>
<li><a href="https://en.wikipedia.org/wiki/Bilateral_filter">There are better alternatives to Gaussian blur for our purpose.</a></li>
</ul>
<p>Reading <a href="https://distill.pub/2017/feature-visualization/">this article will give you an idea of how good feature visualization can look when done right</a>.</p>
<h2>Conclusion</h2>
<p>That's good enough for the scope of this post. I hope you found interpretability to be fun and interesting.</p>
<p>And hey, if you did, don't stop here! I barely scratched the surface of what the field actually revolves around. There are a bunch of resources for you to continue researching:</p>
<ul>
<li><a href="https://distill.pub/2020/circuits/">Distill's Circuits Thread</a>: Great and accessible interpretability papers. Contains <a href="https://distill.pub/2017/feature-visualization">a paper specifically related to feature visualizations</a>.</li>
<li><a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a>: State-of-the-art interpretability papers about transformers and language models.</li>
<li><a href="https://www.arena.education/">ARENA Course</a>: Hands-on practice for AI alignment/interpretability skills.</li>
</ul>
<!--
## Prithvi's Notes

- Explaining the code more explicitly. That may help the more general audience understand exactly what they're doing.
- Maybe add "nutshells", where if people wanted to learn more about a topic, they could open a little box explainer. This is to add information where there could be, but also make it optional.
 -->


    </main>
  </body>
</html>
